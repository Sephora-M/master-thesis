% !TEX root = ../thesis.tex
\chapter{Implementation Details}\label{chap:setup}
We use the deep learning library Caffe \cite{jia2014caffe} in all of our experiments.

\section{Data}
We choose UCF101 dataset because it is a good trade off between number of action classes and the variety of actions. 
This dataset contains 101 action classes and there are at least 100 video clips for each class. 
The whole dataset contains 13,320 video clips, which are divided into 25 groups for each action category.

The person and object bounding boxes are extracted as described in \autoref{sec:detection}.

We retrieve frame images using OpenCV decoder. The flow inputs are obtained using TVL1 algorithm from OpenCV. Both inputs are resized to $ 256 \times 340 $. 

\section{Training}
We fix our network input to be $ 224 \times 224$.
For spatial net, dimension is $ 224 \times 224 \times 3 $, 
while for temporal the net input dimension is $ 224 \times 224 \times 2L $, which is comprised of the concatenation of flow images (in $ x $ and $ y $) in $ L $ consecutive frames.

Data augmentation is a important step in CNN training as it effectively increases data size, elevates variance and prevents overfitting.
Random cropping, horizontal flipping and RBG jittering are common techniques for this purpose.
In this thesis, we applied horizontal flipping and cropping for data augmentation. 
In particular, we adopted the cropping scheme suggested in \cite{wang2015towards}.
This scheme creates $ 5\times 14 $ kinds of croppings candidates from 5 positions (four corners and center of the original image) and 14 size variants.
The width and height of the cropping window is randomly chosen from $ \lbrace 168, 192, 224, 256 \rbrace $, while the most two extreme combinations ($ 168 \times 256 $ and $ 256 \times 168 $) are rejected.
The cropped image patch will be resized to $ 224\times 224 $, which will be used as input for our network.

When training with person or object cue, we use exactly one person bounding box and at least one object bounding boxes for each frame.
For temporal network, we use the union of bounding boxes from each $ L $ frames to determine the input bounding box for the sample sequence.

In order to guarantee sufficient area of person bounding boxes in the cropping window, we reject cropping candidates those overlapping ratios in width and height are lower than $ 0.5 $.
In case no person has been detected in the frame, we use the whole span of the input image as person bounding box so as to keep the gradient scale consistent.

As for objects, we discard object bounding boxes outside cropping area. 
Similar as for person, in absence of valid object bounding boxes, scene bounding box will be used instead.

Since in the current implementation, if the input ROIs are smaller than the specified output dimension $ w \times h $ ($ 7\times7 $), RoiPooling layers returns an all-zero output. 
Thus we enlarge ROIs whose length is smaller than $ 16\times7 $, where 16 is the total stride in conv5-3.

In all experiments, we use the public model VGG16 \cite{simonyan2014very} for initialization and set both \textit{dropout rate} to be $ 0.8 $. 
Without particular specification, RGB stream is trained for 10000 iterations; the \textit{learning rate} is initialized to be $ 1e^{-3} $ and decreased every 4000 iteration.
Whereas for flow, we train 19000 iterations and set the initial \textit{learning rate} to be $ 5e^{-3} $, which is reduced every 8000 iterations.
A batch is assembled by randomly sampling 16 frames from 16 different videos. 
While so, we set the \textit{itersize} to $ 16 $, so that the effective \textit{batchsize} remains $ 256 $.

\section{Testing}
For fair comparison, we follow the same testing routine as in \cite{simonyan2014two}.
Specifically, from each input frame 10 samples generated from 5 crops and horizontal flips. 

However, when using person cue, we discard croppings that have insufficient interceptions with person (same as in training) and the same procedure is repeated for every person track. 

The final classification results from averaging the classification scores of all valid croppings and person tracks.